---
title: "Own_choice_wine_quality"
author: "yumeng"
date: "December 26, 2019"
output: 
 pdf_document:
    fig_width: 10
    fig_height: 8
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy = TRUE,warning = FALSE, message = FALSE)
```

# Summary
### In this study, we will explore some machine learning methods on white wine quality data and try to predict wine quality. The quality score are treated as continuous data, and first we build a linear regression model as our baseline model, then several other methods such as logistic regression, K nearest neighbours etc. The RMSE is the loss function we used in this study, and based on the results we get, random forest has the least RMSE which is 0.64374. And the three most important factor to the final quality predict are volatile acidity, free sulfur dioxide and alcohol. 

#Step1, let's select the work directory and load all the packages
```{r set dir}
my_dir<-choose.dir() #if it doesn't work, please set your own work directory
setwd(my_dir)
print(paste("The director will be used is ",my_dir,sep=":"))

options(digits=5)

packages_used<-c("tidyverse","scales","data.table","caret","rpart","rpart.plot","corrplot","rmarkdown","knitr")

for (i in packages_used){
  if (!requireNamespace(i)){
    install.packages(i)
  }
}

library(tidyverse)
library(scales)
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(rmarkdown)
library(knitr)
```

## Then download the white wine quality data We will use later
```{r download files}
dl <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", destfile = dl, mode = "wb")

wine_white<-fread(dl,sep=";")
setnames(wine_white,c("fixed_acidity","volatile_acidity","citric_acid","residual_sugar","chlorides","free_sulfur_dioxide","total_sulfur_dioxide","density","PH","sulphates", "alcohol","quality"))
save.image("wine_quality.rda")
```

## Load data and create train & test set, 10% of the data will be our test set
```{r echo=TRUE}
load("wine_quality.rda")
set.seed(7,sample.kind = "Rounding") # set.seed(7) if R version is 3.5 or before
index<-createDataPartition(wine_white$quality,times = 1, p=0.1,list = FALSE)
train_white<-wine_white%>%
  slice(-index)
test_white<-wine_white%>%
  slice(index)
```

# Several algorithms will be introduced to see which one has better performance
# Before modeling, let's check the distribution of our wine quality scores

```{r data distribution}
wine_white%>%
  ggplot(aes(quality))+geom_bar(position = "dodge")+
  scale_x_continuous(limits=c(1,10),breaks = seq(1,10,1))
```

#we can see most scores of white wine is 6 
# First a simple regression, let's have a quick look at the linear correlations between each variable

```{r cor chart}
corrplot(cor(wine_white),addCoef.col = "grey",rect.col = "blue",
         method="number",number.cex = 0.8,
         diag = FALSE, tl.pos = "lt", cl.pos = "n",
         tl.cex = 0.8,tl.col = "black")
```

## Define the loss function
```{r}
rmse<-function(x,y){
  sqrt(mean((x-y)^2))
}
```

# Based on the correlation matrix, "alcohol","density" and "chlorides" have higher linear correlation to the quality scores, let's find out if they play important roles in our linear models
#First at first, we build a linear model that used all variables then use step regression to prun the model
```{r linear regression}
white_lm_all<-lm(quality~.,data = train_white)
white_lm<-step(white_lm_all)
rmse_results<-data.frame(method=c("linear model selected variables","linear model all variables"),
    RMSE=c(rmse(predict(white_lm,test_white),test_white$quality),
           rmse(predict(white_lm_all,test_white),test_white$quality)))
knitr::kable(rmse_results)
knitr::kable(broom::tidy(white_lm))
```

# we find after pruning, only density and alcohol are statistically significant in the linear model. 

```{r}
print(paste("The baseline RMSE of white wine is",
         format(RMSE(predict(white_lm,test_white),test_white$quality),digits = 5),sep=":"))
```
 
# Now other methods will be tested
```{r other methods}
set.seed(1,sample.kind = "Rounding") 
models <- c("glm", "svmLinear",
            "knn", "gamLoess", "rf","rpart")    
fits <- lapply(models, function(model){ 
  print(model)
  train(quality ~ ., method = model,trControl=trainControl(method = "cv"),
        data = train_white)
}) 

names(fits) <- models
my_predict<-function(x){predict(x,test_white)}
predict<-sapply(fits,my_predict)
ensemble_pred<-rowMeans(predict)
predict<-as.data.frame(predict)%>%
  mutate(ensemble=ensemble_pred)
auu<-function(x){RMSE(x,test_white$quality)}   
rmses<-apply(predict, 2, auu)
rmses<-as.data.frame(rmses)
rmses$method<-row.names(rmses) 
names(rmses)<-c("RMSE","method")
rmse_results<-bind_rows(rmse_results,rmses)
knitr::kable(rmses)
```

#we could see the prediction more clear in the image below

```{r plot residual}
residual<-predict-test_white$quality
residual%>%
  pivot_longer(cols=1:7,names_to = "models",values_to = "residual")%>%
  ggplot(aes(models,residual))+geom_boxplot()
print(paste("The best performed algorithm is random forest",format(rmses[5,1],5),sep=":"))
```
# Random forest not only has the least RMSE but also narrowest residual range

```{r tune rf}
set.seed(3,sample.kind = "Rounding")
sqtmtry<- round(sqrt(ncol(train_white) - 1))
rfGrid <- expand.grid(mtry = c(round(sqtmtry / 2), sqtmtry, 2 * sqtmtry))
#below code may take a while
rf_fit<-train(quality ~ ., method = "rf", 
              data = train_white,
            tuneGrid = rfGrid,
              nodesize=5,importance=T)

predict_rf<-predict(rf_fit,test_white)

imp<-varImp(rf_fit)
imp$importance%>%
  as.data.frame()%>%
  mutate(var=rownames(imp$importance))%>%
arrange(desc(Overall))%>%
 knitr::kable() 

rmse_results<-bind_rows(rmse_results,
               data.frame(method="tuned rf",
RMSE=RMSE(predict_rf,test_white$quality)))
knitr::kable(rmse_results)
``` 

# It's slighly doing better than the aotmatically tuned rf, the final RMSE is 0.64236

# Conclusion
# In this study, several machine learning methods are applied on white wine quality data to predict wine quality. Random forest has the least RMSE which is 0.64236. And the three most important factor to the final quality predict are volatile acidity, free sulfur dioxide and alcohol. 




